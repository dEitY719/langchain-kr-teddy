{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone\n",
    "\n",
    "Pinecone은 고성능 벡터 데이터베이스로, AI 및 머신러닝 애플리케이션을 위한 효율적인 벡터 저장 및 검색 솔루션입니다. \n",
    "\n",
    "Pinecone, Chroma, Faiss와 같은 벡터 데이터베이스들을 비교해보겠습니다. \n",
    "\n",
    "**Pinecone의 장점**\n",
    "\n",
    "1. 확장성: 대규모 데이터셋에 대해 뛰어난 확장성을 제공합니다.\n",
    "   \n",
    "2. 관리 용이성: 완전 관리형 서비스로, 인프라 관리 부담이 적습니다.\n",
    "   \n",
    "3. 실시간 업데이트: 데이터의 실시간 삽입, 업데이트, 삭제가 가능합니다.\n",
    "   \n",
    "4. 고가용성: 클라우드 기반으로 높은 가용성과 내구성을 제공합니다.\n",
    "   \n",
    "5. API 친화적: RESTful/Python API를 통해 쉽게 통합할 수 있습니다.\n",
    "\n",
    "**Pinecone의 단점**\n",
    "\n",
    "1. 비용: Chroma나 Faiss에 비해 상대적으로 비용이 높을 수 있습니다.\n",
    "   \n",
    "2. 커스터마이징 제한: 완전 관리형 서비스이기 때문에 세부적인 커스터마이징에 제한이 있을 수 있습니다.\n",
    "   \n",
    "3. 데이터 위치: 클라우드에 데이터를 저장해야 하므로, 데이터 주권 문제가 있을 수 있습니다.\n",
    "\n",
    "Chroma나 Faiss와 비교했을 때:\n",
    "\n",
    "- Chroma/FAISS 오픈소스이며 로컬에서 실행 가능하여 초기 비용이 낮고 데이터 제어가 용이합니다. 커스터마이징의 자유도가 높습니다. 하지만 대규모 확장성 면에서는 Pinecone에 비해 제한적일 수 있습니다.\n",
    "\n",
    "선택은 프로젝트의 규모, 요구사항, 예산 등을 고려하여 결정해야 합니다. 대규모 프로덕션 환경에서는 Pinecone이 유리할 수 있지만, 소규모 프로젝트나 실험적인 환경에서는 Chroma나 Faiss가 더 적합할 수 있습니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- [Pinecone 공식 홈페이지](https://docs.pinecone.io/integrations/langchain)\n",
    "- [Pinecone 랭체인](https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH10-VectorStores\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -U langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH10-VectorStores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 업데이트 안내\n",
    "\n",
    "아래의 기능은 커스텀 구현한 내용이므로 아래의 라이브러리를 반드시 업데이트 후 진행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업데이트 명령어\n",
    "# !pip install -U langchain-teddynote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 처리를 위한 불용어 사전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 불용어 사전 가져오기 (추후 토크나이저에 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아',\n",
       " '휴',\n",
       " '아이구',\n",
       " '아이쿠',\n",
       " '아이고',\n",
       " '어',\n",
       " '나',\n",
       " '우리',\n",
       " '저희',\n",
       " '따라',\n",
       " '의해',\n",
       " '을',\n",
       " '를',\n",
       " '에',\n",
       " '의',\n",
       " '가',\n",
       " '으로',\n",
       " '로',\n",
       " '에게',\n",
       " '뿐이다',\n",
       " '의거하여',\n",
       " '근거하여',\n",
       " '입각하여',\n",
       " '기준으로',\n",
       " '예하면',\n",
       " '예를 들면',\n",
       " '예를 들자면',\n",
       " '저',\n",
       " '소인',\n",
       " '소생',\n",
       " '저희',\n",
       " '지말고',\n",
       " '하지마',\n",
       " '하지마라',\n",
       " '다른',\n",
       " '물론',\n",
       " '또한',\n",
       " '그리고',\n",
       " '비길수 없다',\n",
       " '해서는 안된다',\n",
       " '뿐만 아니라',\n",
       " '만이 아니다',\n",
       " '만은 아니다',\n",
       " '막론하고',\n",
       " '관계없이',\n",
       " '그치지 않다',\n",
       " '그러나',\n",
       " '그런데',\n",
       " '하지만',\n",
       " '든간에',\n",
       " '논하지 않다',\n",
       " '따지지 않다',\n",
       " '설사',\n",
       " '비록',\n",
       " '더라도',\n",
       " '아니면',\n",
       " '만 못하다',\n",
       " '하는 편이 낫다',\n",
       " '불문하고',\n",
       " '향하여',\n",
       " '향해서',\n",
       " '향하다',\n",
       " '쪽으로',\n",
       " '틈타',\n",
       " '이용하여',\n",
       " '타다',\n",
       " '오르다',\n",
       " '제외하고',\n",
       " '이 외에',\n",
       " '이 밖에',\n",
       " '하여야',\n",
       " '비로소',\n",
       " '한다면 몰라도',\n",
       " '외에도',\n",
       " '이곳',\n",
       " '여기',\n",
       " '부터',\n",
       " '기점으로',\n",
       " '따라서',\n",
       " '할 생각이다',\n",
       " '하려고하다',\n",
       " '이리하여',\n",
       " '그리하여',\n",
       " '그렇게 함으로써',\n",
       " '하지만',\n",
       " '일때',\n",
       " '할때',\n",
       " '앞에서',\n",
       " '중에서',\n",
       " '보는데서',\n",
       " '으로써',\n",
       " '로써',\n",
       " '까지',\n",
       " '해야한다',\n",
       " '일것이다',\n",
       " '반드시',\n",
       " '할줄알다',\n",
       " '할수있다',\n",
       " '할수있어',\n",
       " '임에 틀림없다',\n",
       " '한다면',\n",
       " '등',\n",
       " '등등',\n",
       " '제',\n",
       " '겨우',\n",
       " '단지',\n",
       " '다만',\n",
       " '할뿐',\n",
       " '딩동',\n",
       " '댕그',\n",
       " '대해서',\n",
       " '대하여',\n",
       " '대하면',\n",
       " '훨씬',\n",
       " '얼마나',\n",
       " '얼마만큼',\n",
       " '얼마큼',\n",
       " '남짓',\n",
       " '여',\n",
       " '얼마간',\n",
       " '약간',\n",
       " '다소',\n",
       " '좀',\n",
       " '조금',\n",
       " '다수',\n",
       " '몇',\n",
       " '얼마',\n",
       " '지만',\n",
       " '하물며',\n",
       " '또한',\n",
       " '그러나',\n",
       " '그렇지만',\n",
       " '하지만',\n",
       " '이외에도',\n",
       " '대해 말하자면',\n",
       " '뿐이다',\n",
       " '다음에',\n",
       " '반대로',\n",
       " '반대로 말하자면',\n",
       " '이와 반대로',\n",
       " '바꾸어서 말하면',\n",
       " '바꾸어서 한다면',\n",
       " '만약',\n",
       " '그렇지않으면',\n",
       " '까악',\n",
       " '툭',\n",
       " '딱',\n",
       " '삐걱거리다',\n",
       " '보드득',\n",
       " '비걱거리다',\n",
       " '꽈당',\n",
       " '응당',\n",
       " '해야한다',\n",
       " '에 가서',\n",
       " '각',\n",
       " '각각',\n",
       " '여러분',\n",
       " '각종',\n",
       " '각자',\n",
       " '제각기',\n",
       " '하도록하다',\n",
       " '와',\n",
       " '과',\n",
       " '그러므로',\n",
       " '그래서',\n",
       " '고로',\n",
       " '한 까닭에',\n",
       " '하기 때문에',\n",
       " '거니와',\n",
       " '이지만',\n",
       " '대하여',\n",
       " '관하여',\n",
       " '관한',\n",
       " '과연',\n",
       " '실로',\n",
       " '아니나다를가',\n",
       " '생각한대로',\n",
       " '진짜로',\n",
       " '한적이있다',\n",
       " '하곤하였다',\n",
       " '하',\n",
       " '하하',\n",
       " '허허',\n",
       " '아하',\n",
       " '거바',\n",
       " '와',\n",
       " '오',\n",
       " '왜',\n",
       " '어째서',\n",
       " '무엇때문에',\n",
       " '어찌',\n",
       " '하겠는가',\n",
       " '무슨',\n",
       " '어디',\n",
       " '어느곳',\n",
       " '더군다나',\n",
       " '하물며',\n",
       " '더욱이는',\n",
       " '어느때',\n",
       " '언제',\n",
       " '야',\n",
       " '이봐',\n",
       " '어이',\n",
       " '여보시오',\n",
       " '흐흐',\n",
       " '흥',\n",
       " '휴',\n",
       " '헉헉',\n",
       " '헐떡헐떡',\n",
       " '영차',\n",
       " '여차',\n",
       " '어기여차',\n",
       " '끙끙',\n",
       " '아야',\n",
       " '앗',\n",
       " '아야',\n",
       " '콸콸',\n",
       " '졸졸',\n",
       " '좍좍',\n",
       " '뚝뚝',\n",
       " '주룩주룩',\n",
       " '솨',\n",
       " '우르르',\n",
       " '그래도',\n",
       " '또',\n",
       " '그리고',\n",
       " '바꾸어말하면',\n",
       " '바꾸어말하자면',\n",
       " '혹은',\n",
       " '혹시',\n",
       " '답다',\n",
       " '및',\n",
       " '그에 따르는',\n",
       " '때가 되어',\n",
       " '즉',\n",
       " '지든지',\n",
       " '설령',\n",
       " '가령',\n",
       " '하더라도',\n",
       " '할지라도',\n",
       " '일지라도',\n",
       " '지든지',\n",
       " '몇',\n",
       " '거의',\n",
       " '하마터면',\n",
       " '인젠',\n",
       " '이젠',\n",
       " '된바에야',\n",
       " '된이상',\n",
       " '만큼\\t어찌됏든',\n",
       " '그위에',\n",
       " '게다가',\n",
       " '점에서 보아',\n",
       " '비추어 보아',\n",
       " '고려하면',\n",
       " '하게될것이다',\n",
       " '일것이다',\n",
       " '비교적',\n",
       " '좀',\n",
       " '보다더',\n",
       " '비하면',\n",
       " '시키다',\n",
       " '하게하다',\n",
       " '할만하다',\n",
       " '의해서',\n",
       " '연이서',\n",
       " '이어서',\n",
       " '잇따라',\n",
       " '뒤따라',\n",
       " '뒤이어',\n",
       " '결국',\n",
       " '의지하여',\n",
       " '기대여',\n",
       " '통하여',\n",
       " '자마자',\n",
       " '더욱더',\n",
       " '불구하고',\n",
       " '얼마든지',\n",
       " '마음대로',\n",
       " '주저하지 않고',\n",
       " '곧',\n",
       " '즉시',\n",
       " '바로',\n",
       " '당장',\n",
       " '하자마자',\n",
       " '밖에 안된다',\n",
       " '하면된다',\n",
       " '그래',\n",
       " '그렇지',\n",
       " '요컨대',\n",
       " '다시 말하자면',\n",
       " '바꿔 말하면',\n",
       " '즉',\n",
       " '구체적으로',\n",
       " '말하자면',\n",
       " '시작하여',\n",
       " '시초에',\n",
       " '이상',\n",
       " '허',\n",
       " '헉',\n",
       " '허걱',\n",
       " '바와같이',\n",
       " '해도좋다',\n",
       " '해도된다',\n",
       " '게다가',\n",
       " '더구나',\n",
       " '하물며',\n",
       " '와르르',\n",
       " '팍',\n",
       " '퍽',\n",
       " '펄렁',\n",
       " '동안',\n",
       " '이래',\n",
       " '하고있었다',\n",
       " '이었다',\n",
       " '에서',\n",
       " '로부터',\n",
       " '까지',\n",
       " '예하면',\n",
       " '했어요',\n",
       " '해요',\n",
       " '함께',\n",
       " '같이',\n",
       " '더불어',\n",
       " '마저',\n",
       " '마저도',\n",
       " '양자',\n",
       " '모두',\n",
       " '습니다',\n",
       " '가까스로',\n",
       " '하려고하다',\n",
       " '즈음하여',\n",
       " '다른',\n",
       " '다른 방면으로',\n",
       " '해봐요',\n",
       " '습니까',\n",
       " '했어요',\n",
       " '말할것도 없고',\n",
       " '무릎쓰고',\n",
       " '개의치않고',\n",
       " '하는것만 못하다',\n",
       " '하는것이 낫다',\n",
       " '매',\n",
       " '매번',\n",
       " '들',\n",
       " '모',\n",
       " '어느것',\n",
       " '어느',\n",
       " '로써',\n",
       " '갖고말하자면',\n",
       " '어디',\n",
       " '어느쪽',\n",
       " '어느것',\n",
       " '어느해',\n",
       " '어느 년도',\n",
       " '라 해도',\n",
       " '언젠가',\n",
       " '어떤것',\n",
       " '어느것',\n",
       " '저기',\n",
       " '저쪽',\n",
       " '저것',\n",
       " '그때',\n",
       " '그럼',\n",
       " '그러면',\n",
       " '요만한걸',\n",
       " '그래',\n",
       " '그때',\n",
       " '저것만큼',\n",
       " '그저',\n",
       " '이르기까지',\n",
       " '할 줄 안다',\n",
       " '할 힘이 있다',\n",
       " '너',\n",
       " '너희',\n",
       " '당신',\n",
       " '어찌',\n",
       " '설마',\n",
       " '차라리',\n",
       " '할지언정',\n",
       " '할지라도',\n",
       " '할망정',\n",
       " '할지언정',\n",
       " '구토하다',\n",
       " '게우다',\n",
       " '토하다',\n",
       " '메쓰겁다',\n",
       " '옆사람',\n",
       " '퉤',\n",
       " '쳇',\n",
       " '의거하여',\n",
       " '근거하여',\n",
       " '의해',\n",
       " '따라',\n",
       " '힘입어',\n",
       " '그',\n",
       " '다음',\n",
       " '버금',\n",
       " '두번째로',\n",
       " '기타',\n",
       " '첫번째로',\n",
       " '나머지는',\n",
       " '그중에서',\n",
       " '견지에서',\n",
       " '형식으로 쓰여',\n",
       " '입장에서',\n",
       " '위해서',\n",
       " '단지',\n",
       " '의해되다',\n",
       " '하도록시키다',\n",
       " '뿐만아니라',\n",
       " '반대로',\n",
       " '전후',\n",
       " '전자',\n",
       " '앞의것',\n",
       " '잠시',\n",
       " '잠깐',\n",
       " '하면서',\n",
       " '그렇지만',\n",
       " '다음에',\n",
       " '그러한즉',\n",
       " '그런즉',\n",
       " '남들',\n",
       " '아무거나',\n",
       " '어찌하든지',\n",
       " '같다',\n",
       " '비슷하다',\n",
       " '예컨대',\n",
       " '이럴정도로',\n",
       " '어떻게',\n",
       " '만약',\n",
       " '만일',\n",
       " '위에서 서술한바와같이',\n",
       " '인 듯하다',\n",
       " '하지 않는다면',\n",
       " '만약에',\n",
       " '무엇',\n",
       " '무슨',\n",
       " '어느',\n",
       " '어떤',\n",
       " '아래윗',\n",
       " '조차',\n",
       " '한데',\n",
       " '그럼에도 불구하고',\n",
       " '여전히',\n",
       " '심지어',\n",
       " '까지도',\n",
       " '조차도',\n",
       " '하지 않도록',\n",
       " '않기 위하여',\n",
       " '때',\n",
       " '시각',\n",
       " '무렵',\n",
       " '시간',\n",
       " '동안',\n",
       " '어때',\n",
       " '어떠한',\n",
       " '하여금',\n",
       " '네',\n",
       " '예',\n",
       " '우선',\n",
       " '누구',\n",
       " '누가 알겠는가',\n",
       " '아무도',\n",
       " '줄은모른다',\n",
       " '줄은 몰랏다',\n",
       " '하는 김에',\n",
       " '겸사겸사',\n",
       " '하는바',\n",
       " '그런 까닭에',\n",
       " '한 이유는',\n",
       " '그러니',\n",
       " '그러니까',\n",
       " '때문에',\n",
       " '그',\n",
       " '너희',\n",
       " '그들',\n",
       " '너희들',\n",
       " '타인',\n",
       " '것',\n",
       " '것들',\n",
       " '너',\n",
       " '위하여',\n",
       " '공동으로',\n",
       " '동시에',\n",
       " '하기 위하여',\n",
       " '어찌하여',\n",
       " '무엇때문에',\n",
       " '붕붕',\n",
       " '윙윙',\n",
       " '나',\n",
       " '우리',\n",
       " '엉엉',\n",
       " '휘익',\n",
       " '윙윙',\n",
       " '오호',\n",
       " '아하',\n",
       " '어쨋든',\n",
       " '만 못하다\\t하기보다는',\n",
       " '차라리',\n",
       " '하는 편이 낫다',\n",
       " '흐흐',\n",
       " '놀라다',\n",
       " '상대적으로 말하자면',\n",
       " '마치',\n",
       " '아니라면',\n",
       " '쉿',\n",
       " '그렇지 않으면',\n",
       " '그렇지 않다면',\n",
       " '안 그러면',\n",
       " '아니었다면',\n",
       " '하든지',\n",
       " '아니면',\n",
       " '이라면',\n",
       " '좋아',\n",
       " '알았어',\n",
       " '하는것도',\n",
       " '그만이다',\n",
       " '어쩔수 없다',\n",
       " '하나',\n",
       " '일',\n",
       " '일반적으로',\n",
       " '일단',\n",
       " '한켠으로는',\n",
       " '오자마자',\n",
       " '이렇게되면',\n",
       " '이와같다면',\n",
       " '전부',\n",
       " '한마디',\n",
       " '한항목',\n",
       " '근거로',\n",
       " '하기에',\n",
       " '아울러',\n",
       " '하지 않도록',\n",
       " '않기 위해서',\n",
       " '이르기까지',\n",
       " '이 되다',\n",
       " '로 인하여',\n",
       " '까닭으로',\n",
       " '이유만으로',\n",
       " '이로 인하여',\n",
       " '그래서',\n",
       " '이 때문에',\n",
       " '그러므로',\n",
       " '그런 까닭에',\n",
       " '알 수 있다',\n",
       " '결론을 낼 수 있다',\n",
       " '으로 인하여',\n",
       " '있다',\n",
       " '어떤것',\n",
       " '관계가 있다',\n",
       " '관련이 있다',\n",
       " '연관되다',\n",
       " '어떤것들',\n",
       " '에 대해',\n",
       " '이리하여',\n",
       " '그리하여',\n",
       " '여부',\n",
       " '하기보다는',\n",
       " '하느니',\n",
       " '하면 할수록',\n",
       " '운운',\n",
       " '이러이러하다',\n",
       " '하구나',\n",
       " '하도다',\n",
       " '다시말하면',\n",
       " '다음으로',\n",
       " '에 있다',\n",
       " '에 달려 있다',\n",
       " '우리',\n",
       " '우리들',\n",
       " '오히려',\n",
       " '하기는한데',\n",
       " '어떻게',\n",
       " '어떻해',\n",
       " '어찌됏어',\n",
       " '어때',\n",
       " '어째서',\n",
       " '본대로',\n",
       " '자',\n",
       " '이',\n",
       " '이쪽',\n",
       " '여기',\n",
       " '이것',\n",
       " '이번',\n",
       " '이렇게말하자면',\n",
       " '이런',\n",
       " '이러한',\n",
       " '이와 같은',\n",
       " '요만큼',\n",
       " '요만한 것',\n",
       " '얼마 안 되는 것',\n",
       " '이만큼',\n",
       " '이 정도의',\n",
       " '이렇게 많은 것',\n",
       " '이와 같다',\n",
       " '이때',\n",
       " '이렇구나',\n",
       " '것과 같이',\n",
       " '끼익',\n",
       " '삐걱',\n",
       " '따위',\n",
       " '와 같은 사람들',\n",
       " '부류의 사람들',\n",
       " '왜냐하면',\n",
       " '중의하나',\n",
       " '오직',\n",
       " '오로지',\n",
       " '에 한하다',\n",
       " '하기만 하면',\n",
       " '도착하다',\n",
       " '까지 미치다',\n",
       " '도달하다',\n",
       " '정도에 이르다',\n",
       " '할 지경이다',\n",
       " '결과에 이르다',\n",
       " '관해서는',\n",
       " '여러분',\n",
       " '하고 있다',\n",
       " '한 후',\n",
       " '혼자',\n",
       " '자기',\n",
       " '자기집',\n",
       " '자신',\n",
       " '우에 종합한것과같이',\n",
       " '총적으로 보면',\n",
       " '총적으로 말하면',\n",
       " '총적으로',\n",
       " '대로 하다',\n",
       " '으로서',\n",
       " '참',\n",
       " '그만이다',\n",
       " '할 따름이다',\n",
       " '쿵',\n",
       " '탕탕',\n",
       " '쾅쾅',\n",
       " '둥둥',\n",
       " '봐',\n",
       " '봐라',\n",
       " '아이야',\n",
       " '아니',\n",
       " '와아',\n",
       " '응',\n",
       " '아이',\n",
       " '참나',\n",
       " '년',\n",
       " '월',\n",
       " '일',\n",
       " '령',\n",
       " '영',\n",
       " '일',\n",
       " '이',\n",
       " '삼',\n",
       " '사',\n",
       " '오',\n",
       " '육',\n",
       " '륙',\n",
       " '칠',\n",
       " '팔',\n",
       " '구',\n",
       " '이천육',\n",
       " '이천칠',\n",
       " '이천팔',\n",
       " '이천구',\n",
       " '하나',\n",
       " '둘',\n",
       " '셋',\n",
       " '넷',\n",
       " '다섯',\n",
       " '여섯',\n",
       " '일곱',\n",
       " '여덟',\n",
       " '아홉',\n",
       " '령',\n",
       " '영']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_teddynote.korean import stopwords\n",
    "\n",
    "# 한글 불용어 사전 불러오기 (불용어 사전 출처: https://www.ranks.nl/stopwords/korean)\n",
    "stopword = stopwords()\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "아래는 일반 문서의 전처리 과정입니다. `ROOT_DIR` 하위에 있는 모든 `.pdf` 파일을 읽어와 `document_lsit` 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import glob\n",
    "\n",
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "split_docs = []\n",
    "\n",
    "# 텍스트 파일을 load -> List[Document] 형태로 변환\n",
    "files = sorted(glob.glob(\"data/*.pdf\"))\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    split_docs.extend(loader.load_and_split(text_splitter))\n",
    "\n",
    "# 문서 개수 확인\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024년 1월호'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinecone 에 DB 저장하기 위한 문서 전처리를 수행합니다. 이 과정에서 `metadata_keys` 를 지정할 수 있습니다.\n",
    "\n",
    "추가로 metadata 를 태깅하고 싶은 경우 사전 처리 작업에서 미리 metadata 를 추가한 뒤 진행합니다.\n",
    "\n",
    "- `split_docs`: 문서 분할 결과를 담은 List[Document] 입니다.\n",
    "- `metadata_keys`: 문서에 추가할 metadata 키를 담은 List 입니다.\n",
    "- `min_length`: 문서의 최소 길이를 지정합니다. 이 길이보다 짧은 문서는 제외합니다.\n",
    "- `use_basename`: 소스 경로를 기준으로 파일명을 사용할지 여부를 지정합니다. 기본값은 `False` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'file_path': 'data/SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 22,\n",
       " 'format': 'PDF 1.4',\n",
       " 'title': 'EU AI 법안에 관한 보고서 초안 주요 내용과 시사점',\n",
       " 'author': 'dj',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'Hancom PDF 1.3.0.480',\n",
       " 'producer': 'Hancom PDF 1.3.0.480',\n",
       " 'creationDate': \"D:20240108125247+09'00'\",\n",
       " 'modDate': \"D:20240108125247+09'00'\",\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata 를 확인합니다.\n",
    "split_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서의 전처리\n",
    "\n",
    "- 필요한 `metadata` 정보를 추출합니다.\n",
    "- 최소 길이 이상의 데이만 필터링 합니다.\n",
    "  \n",
    "- 문서의 `basename` 을 사용할지 여부를 지정합니다. 기본값은 `False` 입니다.\n",
    "  - 여기서 `basename` 이란 파일 경로의 가장 마지막 부분을 의미합니다. \n",
    "  - 예를 들어, `/Users/teddy/data/document.pdf` 의 경우 `document.pdf` 가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'file_path': 'data/SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 22,\n",
       " 'format': 'PDF 1.4',\n",
       " 'title': 'EU AI 법안에 관한 보고서 초안 주요 내용과 시사점',\n",
       " 'author': 'dj',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'Hancom PDF 1.3.0.480',\n",
       " 'producer': 'Hancom PDF 1.3.0.480',\n",
       " 'creationDate': \"D:20240108125247+09'00'\",\n",
       " 'modDate': \"D:20240108125247+09'00'\",\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024년 1월호'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37f811b2e48429fa47d333c2082a9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.community.pinecone import preprocess_documents\n",
    "\n",
    "contents, metadatas = preprocess_documents(\n",
    "    split_docs=split_docs,\n",
    "    metadata_keys=[\"source\", \"page\", \"author\"],\n",
    "    min_length=5,\n",
    "    use_basename=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024년 1월호',\n",
       " '2024년 1월호\\nⅠ. 인공지능 산업 동향 브리프\\n 1. 정책/법제 \\n   ▹ 유럽연합, 세계 최초의 AI 규제 법안에 잠정 합의····························································  1\\n   ▹ 베이징 인터넷법원, AI 생성 이미지의 무단 사용에 저작권법 위반 판결·························  2\\n \\n 2. 기업/산업',\n",
       " '2. 기업/산업 \\n   ▹ 구글, 멀티모달 AI 모델 ‘제미나이’ 공개············································································  3\\n   ▹ 구글 클라우드, 기업용 AI 플랫폼에 이미지 생성 AI ‘이매진 2’ 추가  ·························  4\\n   ▹ 앤스로픽, 20만 개의 토큰을 입력할 수 있는 ‘클로드 2.1’ 공개 ···································  5',\n",
       " '▹ 스태빌리티AI, 동영상 생성 AI ‘스테이블 비디오 디퓨전’ 공개  ····································  6\\n   ▹ 온디바이스 AI, 2024년 주요 기술로 부상할 전망 ··························································  7\\n   ▹ 딜로이트, 2024년 생성 AI 반도체 시장 규모 500억 달러 예측 ···································  8',\n",
       " '▹ IBM과 메타, 개방형 혁신을 위한 ‘AI 얼라이언스’ 결성···················································  9\\n   ▹ 마이크로소프트, 영국 AI 인프라에 2026년까지 32억 달러 투자 계획·························  10\\n   ▹ AMD, 생성 AI에 최적화된 ‘인스팅트 MI300’ 시리즈 발표··········································  11\\n 3. 기술/연구']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorStore 에 저장할 문서 확인\n",
    "contents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'page', 'author'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorStore 에 저장할 metadata 확인\n",
    "metadatas.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'SPRi AI Brief_2024년1월호_F.pdf',\n",
       " 'SPRi AI Brief_2024년1월호_F.pdf']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata 에서 source 를 확인합니다.\n",
    "metadatas[\"source\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(867, 867, 867)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 개수 확인, 소스 개수 확인, 페이지 개수 확인\n",
    "len(contents), len(metadatas[\"source\"]), len(metadatas[\"page\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 키 발급\n",
    "\n",
    "- [링크](https://app.pinecone.io/)\n",
    "- 프로필 - Account - Projects - Starter - API keys - 발급\n",
    "\n",
    "`.env` 파일에 아래와 같이 추가합니다.\n",
    "\n",
    "```\n",
    "PINECONE_API_KEY=\"YOUR_PINECONE_API_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 VectorStore 인덱스 생성\n",
    "\n",
    "Pinecone 의 새로운 인덱스를 생성합니다.\n",
    "\n",
    "![pinecone-01.png](./images/pinecone-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pinecone 인덱스를 생성합니다.\n",
    "\n",
    "**주의사항**\n",
    "- `metric` 은 유사도 측정 방법을 지정합니다. 만약 HybridSearch 를 고려하고 있다면 `metric` 은 `dotproduct` 로 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[create_index]\n",
      "{'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 0}},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_teddynote.community.pinecone import create_index\n",
    "\n",
    "# Pinecone 인덱스 생성\n",
    "pc_index = create_index(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    index_name=\"teddynote-db-index\",  # 인덱스 이름을 지정합니다.\n",
    "    dimension=4096,  # Embedding 차원과 맞춥니다. (OpenAIEmbeddings: 1536, UpstageEmbeddings: 4096)\n",
    "    metric=\"dotproduct\",  # 유사도 측정 방법을 지정합니다. (dotproduct, euclidean, cosine)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 **유료 Pod** 를 사용하는 예시입니다. **유료 Pod** 는 무료 Serverless Pod 대비 더 확장된 기능을 제공합니다.\n",
    "\n",
    "- 참고: https://docs.pinecone.io/guides/indexes/choose-a-pod-type-and-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_teddynote.community.pinecone import create_index\n",
    "from pinecone import PodSpec\n",
    "\n",
    "# Pinecone 인덱스 생성\n",
    "pc_index = create_index(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    index_name=\"teddynote-db-index2\",  # 인덱스 이름을 지정합니다.\n",
    "    dimension=4096,  # Embedding 차원과 맞춥니다. (OpenAIEmbeddings: 1536, UpstageEmbeddings: 4096)\n",
    "    metric=\"dotproduct\",  # 유사도 측정 방법을 지정합니다. (dotproduct, euclidean, cosine)\n",
    "    pod_spec=PodSpec(\n",
    "        environment=\"us-west1-gcp\", pod_type=\"p1.x1\", pods=1\n",
    "    ),  # 유료 Pod 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Encoder 생성\n",
    "\n",
    "- Sparse Encoder 를 생성합니다. \n",
    "- `Kiwi Tokenizer` 와 한글 불용어(stopwords) 처리를 수행합니다.\n",
    "- Sparse Encoder 를 활용하여 contents 를 학습합니다. 여기서 학습한 인코드는 VectorStore 에 문서를 저장할 때 Sparse Vector 를 생성할 때 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import (\n",
    "    create_sparse_encoder,\n",
    "    fit_sparse_encoder,\n",
    ")\n",
    "\n",
    "# 한글 불용어 사전 + Kiwi 형태소 분석기를 사용합니다.\n",
    "sparse_encoder = create_sparse_encoder(stopwords(), mode=\"kiwi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Encoder 에 Corpus 를 학습합니다.\n",
    "\n",
    "- `save_path`: Sparse Encoder 를 저장할 경로입니다. 추후에 `pickle` 형식으로 저장한 Sparse Encoder 를 불러와 Query 임베딩할 때 사용합니다. 따라서, 이를 저장할 경로를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b6374689ea4922a55458347df87da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_sparse_encoder]\n",
      "Saved Sparse Encoder to: ./sparse_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Sparse Encoder 를 사용하여 contents 를 학습\n",
    "saved_path = fit_sparse_encoder(\n",
    "    sparse_encoder=sparse_encoder, contents=contents, save_path=\"./sparse_encoder.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[선택사항] 아래는 나중에 학습하고 저장한 Sparse Encoder 를 다시 불러와야 할 때 사용하는 코드입니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import load_sparse_encoder\n",
    "\n",
    "# 추후에 학습된 sparse encoder 를 불러올 때 사용합니다.\n",
    "sparse_encoder = load_sparse_encoder(\"./sparse_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone: DB Index에 추가 (Upsert)\n",
    "\n",
    "![](./images/pinecone-02.png)\n",
    "\n",
    "- `context`: 문서의 내용입니다.\n",
    "- `page`: 문서의 페이지 번호입니다.\n",
    "- `source`: 문서의 출처입니다.\n",
    "- `values`: Embedder 를 통해 얻은 문서의 임베딩입니다.\n",
    "- `sparse values`: Sparse Encoder 를 통해 얻은 문서의 임베딩입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "upstage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분산 처리를 하지 않고 배치 단위로 문서를 Upsert 합니다. 문서의 양이 많지 않다면 아래의 방식을 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_teddynote.community.pinecone import upsert_documents\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "upsert_documents(\n",
    "    index=pc_index,  # Pinecone 인덱스\n",
    "    namespace=\"teddynote-namespace-01\",  # Pinecone namespace\n",
    "    contents=contents,  # 이전에 전처리한 문서 내용\n",
    "    metadatas=metadatas,  # 이전에 전처리한 문서 메타데이터\n",
    "    sparse_encoder=sparse_encoder,  # Sparse encoder\n",
    "    embedder=upstage_embeddings,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 분산처리를 수행하여 대용량 문서를 빠르게 Upsert 합니다. 대용량 업로드시 활용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210259a65be84dc7a9a45542f7a0b4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "문서 Upsert 중:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 867개의 Vector 가 Upsert 되었습니다.\n",
      "{'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 0},\n",
      "                'teddynote-namespace-01': {'vector_count': 64},\n",
      "                'teddynote-namespace-02': {'vector_count': 931}},\n",
      " 'total_vector_count': 995}\n",
      "CPU times: user 11.6 s, sys: 303 ms, total: 11.9 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_teddynote.community.pinecone import upsert_documents_parallel\n",
    "\n",
    "upsert_documents_parallel(\n",
    "    index=pc_index,  # Pinecone 인덱스\n",
    "    namespace=\"teddynote-namespace-02\",  # Pinecone namespace\n",
    "    contents=contents,  # 이전에 전처리한 문서 내용\n",
    "    metadatas=metadatas,  # 이전에 전처리한 문서 메타데이터\n",
    "    sparse_encoder=sparse_encoder,  # Sparse encoder\n",
    "    embedder=upstage_embeddings,\n",
    "    batch_size=64,\n",
    "    max_workers=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱스 조회/삭제\n",
    "\n",
    "`describe_index_stats` 메서드는 인덱스의 내용에 대한 통계 정보를 제공합니다. 이 메서드를 통해 네임스페이스별 벡터 수와 차원 수 등의 정보를 얻을 수 있습니다.\n",
    "\n",
    "**매개변수**\n",
    "* `filter` (Optional[Dict[str, Union[str, float, int, bool, List, dict]]]): 특정 조건에 맞는 벡터들에 대한 통계만 반환하도록 하는 필터. 기본값은 None\n",
    "* `**kwargs`: 추가 키워드 인자\n",
    "\n",
    "**반환값**\n",
    "* `DescribeIndexStatsResponse`: 인덱스에 대한 통계 정보를 담고 있는 객체\n",
    "\n",
    "**사용 예시**\n",
    "* 기본 사용: `index.describe_index_stats()`\n",
    "* 필터 적용: `index.describe_index_stats(filter={'key': 'value'})`\n",
    "\n",
    "**참고**\n",
    "- metadata 필터링은 유료 사용자에 한하여 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 4096,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 0},\n",
       "                'teddynote-namespace-01': {'vector_count': 64},\n",
       "                'teddynote-namespace-02': {'vector_count': 931}},\n",
       " 'total_vector_count': 995}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 조회\n",
    "pc_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네임스페이스(namespace) 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네임스페이스 'teddynote-namespace-01'의 모든 데이터가 삭제되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.community.pinecone import delete_namespace\n",
    "\n",
    "delete_namespace(\n",
    "    pinecone_index=pc_index,\n",
    "    namespace=\"teddynote-namespace-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 4096,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 0},\n",
       "                'teddynote-namespace-02': {'vector_count': 931}},\n",
       " 'total_vector_count': 931}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 유료 사용자 전용 기능입니다. 유료 사용자는 metadata 필터링을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import delete_by_filter\n",
    "\n",
    "# metadata 필터링(유료 기능) 으로 삭제\n",
    "delete_by_filter(\n",
    "    pinecone_index=pc_index,\n",
    "    namespace=\"teddynote-namespace-02\",\n",
    "    filter={\"source\": {\"$eq\": \"SPRi AI Brief_8월호_산업동향.pdf\"}},\n",
    ")\n",
    "pc_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색기(Retriever) 생성\n",
    "\n",
    "### PineconeKiwiHybridRetriever 초기화 파라미터 설정\n",
    "\n",
    "`init_pinecone_index` 함수와 `PineconeKiwiHybridRetriever` 클래스는 Pinecone을 사용한 하이브리드 검색 시스템을 구현합니다. 이 시스템은 밀집 벡터와 희소 벡터를 결합하여 효과적인 문서 검색을 수행합니다.\n",
    "\n",
    "**Pinecone 인덱스 초기화**\n",
    "\n",
    "`init_pinecone_index` 함수는 Pinecone 인덱스를 초기화하고 필요한 구성 요소를 설정합니다.\n",
    "\n",
    "**매개변수**\n",
    "* `index_name` (str): Pinecone 인덱스 이름\n",
    "* `namespace` (str): 사용할 네임스페이스\n",
    "* `api_key` (str): Pinecone API 키\n",
    "* `sparse_encoder_pkl_path` (str): 희소 인코더 피클 파일 경로\n",
    "* `stopwords` (List[str]): 불용어 리스트\n",
    "* `tokenizer` (str): 사용할 토크나이저 (기본값: \"kiwi\")\n",
    "* `embeddings` (Embeddings): 임베딩 모델\n",
    "* `top_k` (int): 반환할 최대 문서 수 (기본값: 10)\n",
    "* `alpha` (float): 밀집 벡터와 희소 벡터의 가중치 조절 파라미터 (기본값: 0.5)\n",
    "\n",
    "**주요 기능**\n",
    "1. Pinecone 인덱스 초기화 및 통계 정보 출력\n",
    "2. 희소 인코더(BM25) 로딩 및 토크나이저 설정\n",
    "3. 네임스페이스 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init_pinecone_index]\n",
      "{'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 0},\n",
      "                'teddynote-namespace-02': {'vector_count': 931}},\n",
      " 'total_vector_count': 931}\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.community.pinecone import init_pinecone_index\n",
    "\n",
    "pinecone_params = init_pinecone_index(\n",
    "    index_name=\"teddynote-db-index\",  # Pinecone 인덱스 이름\n",
    "    namespace=\"teddynote-namespace-02\",  # Pinecone Namespace\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],  # Pinecone API Key\n",
    "    sparse_encoder_path=\"./sparse_encoder.pkl\",  # Sparse Encoder 저장경로(save_path)\n",
    "    stopwords=stopwords(),  # 불용어 사전\n",
    "    tokenizer=\"kiwi\",\n",
    "    embeddings=UpstageEmbeddings(\n",
    "        model=\"solar-embedding-1-large-query\"\n",
    "    ),  # Dense Embedder\n",
    "    top_k=5,  # Top-K 문서 반환 개수\n",
    "    alpha=0.5,  # alpha=0.75로 설정한 경우, (0.75: Dense Embedding, 0.25: Sparse Embedding)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PineconeKiwiHybridRetriever\n",
    "\n",
    "`PineconeKiwiHybridRetriever` 클래스는 Pinecone과 Kiwi를 결합한 하이브리드 검색기를 구현합니다.\n",
    "\n",
    "**주요 속성**\n",
    "* `embeddings`: 밀집 벡터 변환용 임베딩 모델\n",
    "* `sparse_encoder`: 희소 벡터 변환용 인코더\n",
    "* `index`: Pinecone 인덱스 객체\n",
    "* `top_k`: 반환할 최대 문서 수\n",
    "* `alpha`: 밀집 벡터와 희소 벡터의 가중치 조절 파라미터\n",
    "* `namespace`: Pinecone 인덱스 내 네임스페이스\n",
    "\n",
    "**특징**\n",
    "* 밀집 벡터와 희소 벡터를 결합한 HybridSearch Retriever\n",
    "* 가중치 조절을 통한 검색 전략 최적화 가능\n",
    "* 다양한 동적 metadata 필터링 적용 가능(`search_kwargs` 사용: `filter`, `k`, `rerank`, `rerank_model`, `top_n` 등)\n",
    "\n",
    "**사용 예시**\n",
    "1. `init_pinecone_index` 함수로 필요한 구성 요소 초기화\n",
    "2. 초기화된 구성 요소로 `PineconeKiwiHybridRetriever` 인스턴스 생성\n",
    "3. 생성된 검색기를 사용하여 하이브리드 검색 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PineconeKiwiHybridRetriever` 를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever\n",
    "\n",
    "# 검색기 생성\n",
    "pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앤트로픽의 클로드 하이쿠보다 높은 점수를 기록\n",
      "KEY Contents\n",
      "£ GPT-4o 미니, GPT-3.5 터보 대비 60% 이상 저렴하면서 성능은 우수\n",
      "n 오픈AI는 2024년 7월 28일 가장 비용 효율적인 소형 모델인 ‘GPT-4o 미니’를 출시하고 챗GPT에서 \n",
      "GPT-3.5를 대신해 무료 사용자 및 플러스와 팀 사용자에게 제공한다고 발표\n",
      "∙GPT-4o 미니는 입력 토큰 100만 개당 15센트, 출력 토큰 100만 개당 60센트로 가격이 책정되어, \n",
      "GPT-3.5 터보 대비 60% 이상 저렴\n",
      "{'page': 13.0, 'source': 'SPRi AI Brief_8월호_산업동향.pdf', 'score': 0.4066556}\n",
      "\n",
      "====================\n",
      "\n",
      "1. 정책/법제  \n",
      "2. 기업/산업 \n",
      "3. 기술/연구 \n",
      " 4. 인력/교육\n",
      "11\n",
      "오픈AI, 비용 효율적인 소형 AI 모델 ‘GPT-4o 미니’ 출시\n",
      "n 오픈AI가 GPT-3.5 터보 대비 60% 이상 저렴한 소형 모델 ‘GPT-4o’ 미니를 발표했으며, 저렴한 \n",
      "비용과 낮은 지연 시간으로 다양한 애플리케이션에 활용될 것으로 기대\n",
      "n GPT-4o는 주요 벤치마크에서 GPT-3.5를 앞섰으며, 경쟁 소형 모델인 구글의 제미나이 플래시와 \n",
      "앤트로픽의 클로드 하이쿠보다 높은 점수를 기록\n",
      "KEY Contents\n",
      "{'page': 13.0, 'source': 'SPRi AI Brief_8월호_산업동향.pdf', 'score': 0.39749148}\n",
      "\n",
      "====================\n",
      "\n",
      "대화 기록)를 전달하는 작업, 실시간 고객 지원 챗봇 등 다양한 작업을 처리 가능\n",
      "∙오픈AI는 GPT-4o 미니를 통해 훨씬 저렴한 비용으로 AI를 이용할 수 있게 됨으로써 AI 애플리케이션 \n",
      "범위가 크게 확장될 것으로 기대\n",
      "n GPT-4o 미니는 뛰어난 텍스트 성능과 다중모드 추론을 지원하여 주요 벤치마크에서 GPT-4o에는 \n",
      "미달하지만 GPT-3.5 터보와 기타 소형 모델을 능가하는 결과를 기록\n",
      "∙GPT-4o 미니는 텍스트 지능과 추론 벤치마크인 MMLU에서 82.0%의 점수로 제미나이\n",
      "{'page': 13.0, 'source': 'SPRi AI Brief_8월호_산업동향.pdf', 'score': 0.36650038}\n",
      "\n",
      "====================\n",
      "\n",
      "▹ 오픈AI, 비용 효율적인 소형 AI 모델 ‘GPT-4o 미니’ 출시············································· 11\n",
      "   ▹ 바이두, 최신 AI 모델 ‘어니 4.0 터보’ 공개 ···································································· 12\n",
      "   ▹ 중국 상하이市 법학회, 세계인공지능대회에서 휴머노이드 로봇 거버넌스 지침 발표 ···· 13\n",
      "{'page': 1.0, 'source': 'SPRi AI Brief_8월호_산업동향.pdf', 'score': 0.36321446}\n",
      "\n",
      "====================\n",
      "\n",
      "1. 정책/법제  \n",
      "2. 기업/산업 \n",
      "3. 기술/연구 \n",
      " 4. 인력/교육\n",
      "7\n",
      "오픈AI, 사람과 자연스러운 실시간 대화가 가능한 ‘GPT-4o’ 출시\n",
      "n 오픈AI가 응답시간이 최소 0.23초, 평균 0.32초에 불과해 사람과 실시간 음성 대화가 가능한 \n",
      "신규 AI 모델 ‘GPT-4o’를 무료로 공개\n",
      "n GPT-4o는 API로 제공 시 기존 GPT 모델보다 처리속도가 2배 빠르고 비용은 절반 수준이며, \n",
      "다국어, 오디오, 이미지 관련 벤치마크 테스트에서 최고 수준의 성능을 기록\n",
      "KEY Contents\n",
      "{'page': 9.0, 'source': 'SPRi AI Brief_6월호_산업동향 최종.pdf', 'score': 0.32560313}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\"gpt-4o 미니 출시 관련 정보에 대해서 알려줘\")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동적 `search_kwargs` 사용\n",
    "- `k`: 반환할 최대 문서 수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앤트로픽의 클로드 하이쿠보다 높은 점수를 기록\n",
      "KEY Contents\n",
      "£ GPT-4o 미니, GPT-3.5 터보 대비 60% 이상 저렴하면서 성능은 우수\n",
      "n 오픈AI는 2024년 7월 28일 가장 비용 효율적인 소형 모델인 ‘GPT-4o 미니’를 출시하고 챗GPT에서 \n",
      "GPT-3.5를 대신해 무료 사용자 및 플러스와 팀 사용자에게 제공한다고 발표\n",
      "∙GPT-4o 미니는 입력 토큰 100만 개당 15센트, 출력 토큰 100만 개당 60센트로 가격이 책정되어, \n",
      "GPT-3.5 터보 대비 60% 이상 저렴\n",
      "{'page': 13.0, 'source': 'SPRi AI Brief_8월호_산업동향.pdf', 'score': 0.4066556}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\n",
    "    \"gpt-4o 미니 출시 관련 정보에 대해서 알려줘\", search_kwargs={\"k\": 1}\n",
    ")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동적 `search_kwargs` 사용\n",
    "- `alpha`: 밀집 벡터와 희소 벡터의 가중치 조절 파라미터. 0과 1 사이의 값을 지정합니다. `0.5` 가 기본값이고, 1에 가까울수록 dense 벡터의 가중치가 높아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 앤스로픽은 개발자 콘솔도 업데이트하여, 개발자가 여러 프롬프트를 생성하고 테스트함으로써 \n",
      "클로드를 최적화할 수 있도록 지원\n",
      "∙앤스로픽은 사용자가 클로드에 맞춤형 지침을 제공할 수 있는 ‘시스템 프롬프트’ 기능도 도입할 \n",
      "예정으로, 이 기능은 클로드가 사용자 요구에 맞춤화된 방식으로 체계적 응답을 할 수 있도록 지원\n",
      "☞ 출처 : Anthropic, Introducing Claude 2.1, 2023.11.21.\n",
      "{'page': 7.0, 'source': 'SPRi AI Brief_2024년1월호_F.pdf', 'score': 0.3507582}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\n",
    "    \"앤스로픽\", search_kwargs={\"alpha\": 1, \"k\": 1}\n",
    ")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 정책/법제  \n",
      "2. 기업/산업 \n",
      "3. 기술/연구 \n",
      " 4. 인력/교육\n",
      "11\n",
      "앤스로픽 연구 결과, AI도 사람처럼 의도적으로 거짓말 가능\n",
      "n 앤스로픽이 특정 프롬프트가 주어지면 사용자를 속이고 악성코드를 출력하는 등, 악의적 \n",
      "행동을 할 수 있는 LLM을 연구\n",
      "n 앤스로픽에 따르면 기만적 행동을 하는 LLM에 대한 안전성 훈련 이후에도 이를 제거할 수 \n",
      "없었으며 오히려 해당 행동을 유발하는 프롬프트에 대한 반응의 정확도가 향상됨 \n",
      "n\n",
      "KEY Contents\n",
      "£ 앤스로픽, 사용자를 속이고 악성코드를 출력하는 LLM 연구\n",
      "{'page': 13.0, 'source': 'SPRi AI Brief_2024년3월호_F.pdf', 'score': 0.73482233}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\n",
    "    \"앤스로픽\", search_kwargs={\"alpha\": 0, \"k\": 1}\n",
    ")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metadata 필터링**\n",
    "\n",
    "![](./images/pinecone-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동적 `search_kwargs` 사용\n",
    "- `filter`: metadata 필터링 적용\n",
    "\n",
    "(예시) `page` 가 5보다 작은 문서만 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▹ 앤스로픽, 최신 AI 모델 ‘클로드 3.5 소네트’ 출시··························································· 10\n",
      "   ▹ 프랑스의 미스트랄AI, 6억 유로 규모의 투자 유치  ························································ 11\n",
      "{'page': 1.0, 'source': 'SPRi AI Brief_7월호_산업동향.pdf', 'score': 0.3138861}\n",
      "\n",
      "====================\n",
      "\n",
      "2. 기업/산업 \n",
      "   ▹ 구글, 멀티모달 AI 모델 ‘제미나이’ 공개············································································  3\n",
      "   ▹ 구글 클라우드, 기업용 AI 플랫폼에 이미지 생성 AI ‘이매진 2’ 추가  ·························  4\n",
      "   ▹ 앤스로픽, 20만 개의 토큰을 입력할 수 있는 ‘클로드 2.1’ 공개 ···································  5\n",
      "{'page': 1.0, 'source': 'SPRi AI Brief_2024년1월호_F.pdf', 'score': 0.22380425}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\n",
    "    \"앤스로픽의 claude 출시 관련 내용을 알려줘\",\n",
    "    search_kwargs={\"filter\": {\"page\": {\"$lt\": 5}}, \"k\": 2},\n",
    ")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동적 `search_kwargs` 사용\n",
    "- `filter`: metadata 필터링 적용\n",
    "\n",
    "(예시) `source` 가 `SPRi AI Brief_8월호_산업동향.pdf` 문서내 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▹ 앤스로픽, 최신 AI 모델 ‘클로드 3.5 소네트’ 출시··························································· 10\n",
      "   ▹ 프랑스의 미스트랄AI, 6억 유로 규모의 투자 유치  ························································ 11\n",
      "{'page': 1.0, 'source': 'SPRi AI Brief_7월호_산업동향.pdf', 'score': 0.35717148}\n",
      "\n",
      "====================\n",
      "\n",
      "1. 정책/법제  \n",
      "2. 기업/산업 \n",
      "3. 기술/연구 \n",
      " 4. 인력/교육\n",
      "9\n",
      "앤스로픽, 최신 AI 모델 ‘클로드 3.5 소네트’ 출시 \n",
      "n 앤스로픽이 최신 AI 모델 ‘클로드 3.5 소네트’을 공개하고 이전 모델 중 가장 고성능 모델인 \n",
      "‘클로드 3 오퍼스’나 오픈AI의 최신 모델 ‘GPT-4o’보다 성능이 뛰어나다고 강조\n",
      "n 앤스로픽은 클로드로 생성한 코딩이나 문서 등을 실시간으로 확인해 편집과 같은 작업을 할 수 \n",
      "있는 ‘아티팩트’ 신기능도 출시\n",
      "KEY Contents\n",
      "{'page': 11.0, 'source': 'SPRi AI Brief_7월호_산업동향.pdf', 'score': 0.35150543}\n",
      "\n",
      "====================\n",
      "\n",
      "있는 ‘아티팩트’ 신기능도 출시\n",
      "KEY Contents\n",
      "£ 클로드 3.5 소네트, 이전 대표모델 클로드 3 오퍼스와 오픈AI의 GPT-4o 능가\n",
      "n 앤스로픽이 2024년 6월 21일 ‘클로드(Claude) 3.5’ 모델군 중 첫 번째로 ‘소네트(Sonnet)’를 공개\n",
      "했으며, 하반기에 경량 모델 ‘하이쿠(Haiku)’와 가장 강력한 ‘오퍼스(Opus)’를 출시할 계획\n",
      "∙소네트는 클로드 웹사이트(Claude.ai) 및 iOS 앱에서 무료로 제공되며, 유료 가입자에게는 이용 \n",
      "한도를 대폭 넓혀서 제공\n",
      "{'page': 11.0, 'source': 'SPRi AI Brief_7월호_산업동향.pdf', 'score': 0.34038982}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\n",
    "    \"앤스로픽의 claude 3.5 출시 관련 내용을 알려줘\",\n",
    "    search_kwargs={\n",
    "        \"filter\": {\"source\": {\"$eq\": \"SPRi AI Brief_7월호_산업동향.pdf\"}},\n",
    "        \"k\": 3,\n",
    "    },\n",
    ")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking 적용\n",
    "\n",
    "아직은 `pre` 기능입니다.\n",
    "\n",
    "- 동적 reranking 기능을 구현해 놓았지만, pinecone 라이브러리 의존성에 문제가 있을 수 있습니다.\n",
    "- 따라서, 아래 코드는 향후 의존성 해결 후 원활하게 동작할 수 있습니다.\n",
    "\n",
    "참고 문서: https://docs.pinecone.io/guides/inference/rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\n",
    "    \"앤스로픽의 클로드 소넷\",\n",
    "    search_kwargs={\"rerank\": True, \"rerank_model\": \"bge-reranker-v2-m3\", \"top_n\": 3},\n",
    ")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seoulrnd-graphrag-gR8UbrDn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
